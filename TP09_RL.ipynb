{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2CSSID-TP09. Apprentissage par renforcement\n",
    "\n",
    "Dans ce TP, nous allons implémenter un algorithme basé sur l'apprentissage par renforcement pour le problème du taxi et passager.\n",
    "Nous allons implémener l'agent (générique) qui se base sur Q-Learning et l'environnement (spécifique) qui attribue le récompenses en se basant sur les actions de l'agent.\n",
    "Ensuite, nous allons comparer entre l'exporation et l'exploitation.\n",
    "Aussi, nous allons tester l'effet du taux d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.24.3', '1.5.0', '3.6.2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.__version__, pd.__version__, matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Type, Dict, Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Réalisation des algorithmes\n",
    "\n",
    "Dans cette partie, nous allons implémenter un algorithme d'apprentissage par renforcement.\n",
    "Deux classes seront implémentées : l'agent et l'environnement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1. Agent\n",
    "\n",
    "Ici, nous commençons par réaliser les fonctions de l'agent.\n",
    "Nous allons impémenter Q-learning où l'agent possède une matrice des états et des actions.\n",
    "\n",
    "#### I.1.1. Création de la table Q\n",
    "\n",
    "Etant donné $n$ états et $m$ actions, nous devons créer une matrice $Q[n, m]$ initialisée à $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Création de la table Q\n",
    "def creer_Q(nbr_etats: int, nbr_actions: int) -> np.ndarray:\n",
    "    return np.zeros((nbr_etats, nbr_actions))\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[0., 0., 0.],\n",
    "#        [0., 0., 0.],\n",
    "#        [0., 0., 0.],\n",
    "#        [0., 0., 0.],\n",
    "#        [0., 0., 0.]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "Q5_3 = creer_Q(5, 3)\n",
    "\n",
    "Q5_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.2. Exploration et Exploration de la table Q\n",
    "\n",
    "Dans les deux fonctions, il faut choisir un entier entre $0$ et $m$ (nombre des actions).\n",
    "Dans l'exploration, ce nombre est choisi aléatoirement. \n",
    "Dans l'exploitation, ce nombre est l'action qui a une valeur max parmi celles de l'état courant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Exploration\n",
    "def exploration(Q: np.ndarray) -> int:\n",
    "    return np.random.choice(len(Q[1]))\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# un nombre aléatoire dans {0, 1, 2}\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "exploration(Q5_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 0, 1, 2, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Exploitation\n",
    "def exploitation(Q: np.ndarray, etat: int) -> int:\n",
    "    m = np.max(Q, axis=1)\n",
    "    return list(np.where(Q[etat] == m[etat]))[0][0]\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (2, 0, 1, 2, 1)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "Q_t = np.array([\n",
    "    [0.1, 0.2, 0.3],\n",
    "    [1.0, 0.5, 0.7],\n",
    "    [0.5, 1.0, 0.8],\n",
    "    [0.2, 0.8, 0.9],\n",
    "    [0.2, 1.0, 0.3]\n",
    "])\n",
    "\n",
    "exploitation(Q_t, 0), exploitation(Q_t, 1), exploitation(Q_t, 2), exploitation(Q_t, 3), exploitation(Q_t, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def choisir_action(Q: np.ndarray, etat: int, epsilon: float=0.2) -> int:\n",
    "    if np.random.random() < epsilon:\n",
    "        return exploration(Q)\n",
    "    else:\n",
    "        return exploitation(Q, etat)\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# Soit 2 soit un autre nombre dans {0, 1, 2}\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "choisir_action(Q_t, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.3. Mise à jours de la table Q\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) = Q(s_t, a_t) + \\alpha * (r + \\gamma * \\max_a Q(s_{t+1}, a) - Q(s_t, a_t))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1 , 0.2 , 0.3 ],\n",
       "       [1.  , 0.5 , 1.58],\n",
       "       [0.5 , 1.  , 0.8 ],\n",
       "       [0.2 , 0.8 , 0.9 ],\n",
       "       [0.2 , 1.  , 0.3 ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Mise à jours de la table Q\n",
    "def mettre_ajour_Q(Q: np.ndarray, etat: int, netat: int, action: int, alpha: float, r: float, gamma: float) -> np.ndarray:\n",
    "    new_Q = Q.copy()\n",
    "    new_Q[etat,action] = Q[etat,action] + alpha*(r + gamma*exploitation(Q, netat) - Q[etat,action])\n",
    "    return new_Q\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[0.1 , 0.2 , 0.3 ],\n",
    "#        [1.  , 0.5 , 1.58],\n",
    "#        [0.5 , 1.  , 0.8 ],\n",
    "#        [0.2 , 0.8 , 0.9 ],\n",
    "#        [0.2 , 1.  , 0.3 ]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "mettre_ajour_Q(Q_t, 1, 2, 2, 0.2, 5, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.4. La classe Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.  ,  4.  ,  0.  ],\n",
       "       [-0.36, -0.2 , -2.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ],\n",
       "       [-0.2 ,  0.4 ,  0.  ],\n",
       "       [ 2.  ,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Agent:\n",
    "    def __init__(self, nbr_etats: int, nbr_actions: int, alpha: float, epsilon=0.2):\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = creer_Q(nbr_etats, nbr_actions)\n",
    "    \n",
    "    def set_etat(self, etat: int):\n",
    "        self.etat = etat\n",
    "        self.action = 0\n",
    "        \n",
    "    def choisir_action(self):\n",
    "        self.action = choisir_action(self.Q, self.etat, self.epsilon)\n",
    "        return self.action\n",
    "    \n",
    "    def appliquer(self, netat: int, action: int, r: float, gamma: float):\n",
    "        self.Q = mettre_ajour_Q(self.Q, self.etat, netat, self.action, self.alpha, r, gamma)\n",
    "        self.etat = netat\n",
    "        \n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[-2.  ,  4.  ,  0.  ],\n",
    "#        [-0.36, -0.2 , -2.  ],\n",
    "#        [ 0.  ,  0.  ,  0.  ],\n",
    "#        [-0.2 ,  0.4 ,  0.  ],\n",
    "#        [ 2.  ,  0.  ,  0.  ]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "netats_rs = [(0, -1), (1, -10), (3, -1), (1, 2), (4, -1), (1, 10), (1, -10), (0, -1), (0, 20)]\n",
    "\n",
    "agent = Agent(5, 3, 0.2, epsilon=0.) # exploitation: pour qu'il soit déterministe\n",
    "agent.set_etat(3) # etat initial = 3\n",
    "\n",
    "for netat, r in netats_rs:\n",
    "    action = agent.choisir_action()\n",
    "    # FeedBack de l'environnement (netat, r)\n",
    "    agent.appliquer(netat, action, r, gamma=0.5)\n",
    "    \n",
    "\n",
    "agent.Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Environnement\n",
    "\n",
    "Ici, nous allons implémenter le problème du taxi et passager : (https://arxiv.org/pdf/cs/9905014.pdf).\n",
    "Notre environnement est un espace divisé en $nb_l$ lignes et $nb_c$ colonnes pour indiquer la position.\n",
    "Il contient, aussi, un nombre d'arrêts $b_a$. \n",
    "La position du taxi est encodée en utilisant le numéro de la ligne et de la colonne (les coordonnées).\n",
    "La destination est le numéro de l'arrêt ($0<= dst < nb_a$).\n",
    "La position du passager est représentée par le numéro de l'arrêt ($0 <= psg < nb_a$) plus un numéro $psg = nb_a$ indiquant que le passager est à l'intérieur du taxi.\n",
    "\n",
    "#### I.2.1. Encodage et décodage des états\n",
    "\n",
    "**Rien à programmer ici**\n",
    "\n",
    "Ici, nous avons deux fonctions : une qui encode l'état en se basant sur la position du taxi, le numéro de l'arrêt de démarrage, le numéro de l'arrêt destinataire, le nombre des colonnes, des lignes et des arrêts.\n",
    "L'autre fonction décode un état en position du taxi : ligne et colonne plus l'arrêt du passager et l'arrêt destinataire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encoder_etat(pos: Tuple[int, int], psg: int, dst: int, nb_l: int, nb_c: int, nb_a: int) -> int:\n",
    "    return (pos[0] * nb_c + pos[1]) * (nb_a + 1) * nb_a + (psg * nb_a + dst)\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# 153\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "encoder_etat((1, 2), 3, 1, 5, 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decoder_etat(etat: int, nb_l:int, nb_c: int, nb_a: int) -> Tuple[int, int, int, int]:\n",
    "    nb_pa = (nb_a + 1) * nb_a # nombre max des positions passager * arret par case\n",
    "    \n",
    "    pa = etat % nb_pa # position passager * position arret\n",
    "    dst = pa % nb_a\n",
    "    psg = pa// nb_a\n",
    "    \n",
    "    lc = etat // nb_pa # ligne * colonne\n",
    "    l = lc // nb_c\n",
    "    c = lc % nb_c\n",
    "    \n",
    "    return l, c, psg, dst\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (1, 2, 3, 1)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "decoder_etat(153, 5, 5, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.2. Calculer la récompense\n",
    "\n",
    "La fonction de récompense a comme entrée : \n",
    "\n",
    "- etat : l'état courant de l'agent\n",
    "- action : numéro de l'action choisie par l'agent \n",
    "- nb_l, nb_c : nombre des lignes et des colonnes dans l'environnement\n",
    "- arrets : une liste des posittions des arrets. La position est encodée sous forme d'un tuple (x, y). P.S. Les tuples sont hashables ; donc on peut vérifier leur existance dans la liste en utilisant l'opérateur \"in\".\n",
    "- bar : un dictionnaire {pos: list entiers}. Si une position existe, on aura une liste des actions interdites (actions de positionnement).\n",
    "\n",
    "La fonction doit retourner : la récompense, l'état suivant et un booléan qui indique la fin.\n",
    "\n",
    "**La récompense**\n",
    "\n",
    "- Pour chaque action exécutée, une récompense de -1 est attribuée\n",
    "- Si l'agent essaye de déposer ou prendre un passager illégalement, une récompense de -10 est attribuée en plus. L'action \"déposer\" est considérée illégale si le passager n'est pas dans le taxi ou si le lieu de dépot n'est pas l'arrêt destinataire. L'action \"prendre\" est considérée illégale si le passager n'est pas dans la position actuelle ou il est déjà dans la voiture.\n",
    "- Si l'agent dépose le passager dans l'arrêt destinaire, il aura une récompense de +20 en plus.\n",
    "\n",
    "**L'état suivant**\n",
    "\n",
    "- L'état suivant est celui actuel sauf dans les cas suivants\n",
    "- Si le passager monte dans le taxi, l'index du passager sera \"nb_a\" (dans la voiture). \n",
    "- Si le passager arrive à sa destination en sortant du taxi, l'index du passager sera \"dst\". La fin sera True.\n",
    "- Dans le cas d'une action de postionnement ({0, 1, 2, 3}), si la position n'est pas dans celles du barrière \"bar\" ou elle existe mais l'action n'existe pas dans la liste des actions interdites, la nouvelle position et le nouveau état sont calculés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 2), (1, 2), (1, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# En se basant sur l'action et la position, retourner la nouvelle position.\n",
    "# Cette fonction ne prend pas en considération les contraintes \n",
    "def repositionner(pos: Tuple[int, int], action: int) -> Tuple[int, int]:  \n",
    "    if action == 0: # gauche\n",
    "        return pos[0], pos[1]  - 1\n",
    "    if action == 1: # droit\n",
    "        return pos[0], pos[1]  + 1\n",
    "    if action == 2: # avant \n",
    "        return pos[0] + 1, pos[1]\n",
    "    if action == 3: # arrière\n",
    "        return pos[0] - 1, pos[1]\n",
    "    \n",
    "    return pos # action > 3\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# ((2, 2), (1, 2), (1, 1))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "repositionner((1, 2), 2), repositionner((1, 2), 4), repositionner((1, 2), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-11, 56, False),\n",
       " (-11, 44, False),\n",
       " (-1, 96, False),\n",
       " (19, 0, True),\n",
       " (-11, 4, False),\n",
       " (-11, 56, False),\n",
       " (-11, 44, False),\n",
       " (-1, 44, False),\n",
       " (-1, 284, False),\n",
       " (-1, 224, False)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Récompense\n",
    "def calculer_recompense(etat: int, action: int, \n",
    "                        nb_l:int, nb_c: int, \n",
    "                        arrets: List[Tuple[int, int]],\n",
    "                        bar: Dict[Tuple[int, int], Set[int]]) -> Tuple[float, int, bool]:\n",
    "    \n",
    "    recompense = -1 # Toujours on applique cette récompense\n",
    "    netat = etat\n",
    "    fin = False\n",
    "    nb_a = len(arrets)\n",
    "    l, c, psg, dst = decoder_etat(etat, nb_l, nb_c, nb_a)\n",
    "    pos = (l, c)\n",
    "    # Compléter ici\n",
    "    if action == 5: #déposer\n",
    "        if psg != pos: # passanger n'est pas dans le taxi\n",
    "            recompense-=10\n",
    "            else if pos != dst: # n'est pas destination \n",
    "                recompense-=10\n",
    "    return recompense, netat, fin\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# [(-11, 56, False),\n",
    "#  (-11, 44, False),\n",
    "#  (-1, 96, False),\n",
    "#  (19, 0, True),\n",
    "#  (-11, 4, False),\n",
    "#  (-11, 56, False),\n",
    "#  (-11, 44, False),\n",
    "#  (-1, 44, False),\n",
    "#  (-1, 284, False),\n",
    "#  (-1, 224, False)]\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "nb_l, nb_c = 5, 5\n",
    "arrets = [(0,0), (0,4), (4,0), (4,3)]\n",
    "nb_a = len(arrets)\n",
    "barrieres = {\n",
    "    (0, 1): set([1]), # barrière à droit\n",
    "    (0, 2): set([0]), # barrière à gauche\n",
    "    (3, 0): set([1]), # barrière à droit\n",
    "    (4, 0): set([1]), # barrière à droit\n",
    "    (3, 1): set([0]), # barrière à gauche\n",
    "    (4, 1): set([0]), # barrière à gauche\n",
    "    (3, 2): set([1]), # barrière à droit\n",
    "    (4, 2): set([1]), # barrière à droit\n",
    "    (3, 3): set([0]), # barrière à gauche\n",
    "    (4, 3): set([0]), # barrière à gauche\n",
    "}\n",
    "\n",
    "resultats = []\n",
    "\n",
    "tests = [# (etat, action)\n",
    "    # action = 4 (prendre un passager)\n",
    "    (encoder_etat((0, 2), 4, 0, nb_l, nb_c, nb_a), 4), # pos=(0,2); psg=dans la voiture; dst=arrêt0(0, 0)\n",
    "    (encoder_etat((0, 2), 1, 0, nb_l, nb_c, nb_a), 4), # pos=(0,2); psg=arrêt1; dst=arrêt0(0, 0)\n",
    "    (encoder_etat((0, 4), 1, 0, nb_l, nb_c, nb_a), 4), # pos=arrêt1; psg=arrêt1; dst=arrêt0(0, 0)\n",
    "    # action = 5 (déposer un passager)\n",
    "    (encoder_etat((0, 0), 4, 0, nb_l, nb_c, nb_a), 5), # pos=arrêt0; psg=dans la voiture; dst=arrêt0(0, 0)\n",
    "    (encoder_etat((0, 0), 1, 0, nb_l, nb_c, nb_a), 5), # pos=arrêt0; psg=arrêt1; dst=arrêt0(0, 0)\n",
    "    (encoder_etat((0, 2), 4, 0, nb_l, nb_c, nb_a), 5), # pos=(0, 2); psg=dans la voiture; dst=arrêt0(0, 0)\n",
    "    (encoder_etat((0, 2), 1, 0, nb_l, nb_c, nb_a), 5), # pos=(0, 2); psg=arrêt1; dst=arrêt0(0, 0)\n",
    "    # action = 0 (aller à gauche)\n",
    "    (encoder_etat((0, 2), 1, 0, nb_l, nb_c, nb_a), 0), # il existe une barrière à gauche\n",
    "    (encoder_etat((3, 0), 1, 0, nb_l, nb_c, nb_a), 0), # il existe une barrière à droite\n",
    "    (encoder_etat((2, 2), 1, 0, nb_l, nb_c, nb_a), 0), # il n'existe aucune barrière\n",
    "]\n",
    "\n",
    "for etat, action in tests:\n",
    "    resultats.append(calculer_recompense(etat, action, nb_l, nb_c, arrets, barrieres))\n",
    "\n",
    "resultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.3. La classe Envinonnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIN\n"
     ]
    }
   ],
   "source": [
    "import time, sys\n",
    "from IPython.display import HTML, display, clear_output\n",
    "\n",
    "class TaxiEnv():\n",
    "    def __init__(self, nb_l:int, nb_c: int, \n",
    "                 arrets: List[Tuple[int, int]], \n",
    "                 bar: Dict[Tuple[int, int], Set[int]], gamma: float = 0.5):\n",
    "        self.actions = ['gauche', 'droit', 'avant', 'arriere', 'prendre', 'deposer']\n",
    "        self.arrets = arrets\n",
    "        self.nb_l = nb_l\n",
    "        self.nb_c = nb_c\n",
    "        self.nb_etats = nb_l * nb_c * (len(arrets) + 1) * len(arrets)\n",
    "        self.bar = bar\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        for i in range(nb_l):\n",
    "            pos = (i, 0)\n",
    "            if pos not in bar:\n",
    "                bar[pos] = set()\n",
    "            bar[pos].add(0) # on ne peut pas aller à gauche\n",
    "            pos = (i, nb_c-1)\n",
    "            if pos not in bar:\n",
    "                bar[pos] = set()\n",
    "            bar[pos].add(1) # on ne peut pas aller à droit\n",
    "        for j in range(nb_c):\n",
    "            pos = (0, j)\n",
    "            if pos not in bar:\n",
    "                bar[pos] = set()\n",
    "            bar[pos].add(3) # on ne peut pas aller en avant\n",
    "            pos = (nb_l-1, j)\n",
    "            if pos not in bar:\n",
    "                bar[pos] = set()\n",
    "            bar[pos].add(2) # on ne peut pas aller en arrière\n",
    "            \n",
    "        \n",
    "    def ajouter_agent(self, alpha: float, epsilon=0.2):\n",
    "        self.agent = Agent(self.nb_etats, len(self.actions), alpha, epsilon=epsilon)\n",
    "    \n",
    "    def encoder_etat(self, pos: Tuple[int, int], psg: int, dst: int):\n",
    "        return encoder_etat(pos, psg, dst, self.nb_l, self.nb_c, len(self.arrets))\n",
    "    \n",
    "    def decoder_etat(self, etat: int) -> Tuple[int, int, int]:\n",
    "        return decoder_etat(etat, self.nb_l, self.nb_c, len(self.arrets))\n",
    "    \n",
    "    def initialiser(self, pos: Tuple[int, int], psg: int, dst: int):\n",
    "        etat = self.encoder_etat(pos, psg, dst)\n",
    "        self.agent.set_etat(etat)\n",
    "    \n",
    "    def transporter(self, plot=False):\n",
    "        \n",
    "        nb_l = self.nb_l\n",
    "        nb_c = self.nb_c\n",
    "        arrets = self.arrets\n",
    "        bar = self.bar\n",
    "        nb_a = len(arrets)\n",
    "        actions = self.actions\n",
    "        \n",
    "        etat = self.agent.etat\n",
    "        \n",
    "        etapes = []\n",
    "        fin = False\n",
    "        rt = 0\n",
    "        \n",
    "        while not fin:\n",
    "            action = self.agent.choisir_action()\n",
    "            r, netat, fin = calculer_recompense(etat, action, nb_l, nb_c, arrets, bar)\n",
    "            etapes.append((self.decoder_etat(etat), actions[action], r, fin))\n",
    "            self.agent.appliquer(netat, action, r, self.gamma)\n",
    "            if plot:\n",
    "                rt += r\n",
    "                html = self.dessiner()\n",
    "                html += '<div class=\"cont\">'\n",
    "                html += f'<p>Etape: {len(etapes)}</p>'\n",
    "                html += f'<p>Etat: {etat}</p>'\n",
    "                html += f'<p>Action: {actions[action]}</p>'\n",
    "                html += f'<p>Récompense: {r}</p>'\n",
    "                html += f'<p>Récompense totale: {rt}</p>'\n",
    "                html += '</div>'\n",
    "                time.sleep(0.5)\n",
    "                clear_output(wait=True)\n",
    "                display(HTML(html))\n",
    "                sys.stdout.flush()\n",
    "            etat = netat\n",
    " \n",
    "        return etapes\n",
    "    \n",
    "    def dessiner(self):\n",
    "        bordures = ['l', 'r', 'b', 't']\n",
    "        \n",
    "        nb_a = len(self.arrets)\n",
    "        \n",
    "        if hasattr(self.agent, 'etat'):\n",
    "            l, c, psg, dst = decoder_etat(self.agent.etat, self.nb_l, self.nb_c, nb_a)\n",
    "        else:\n",
    "            l, c, psg, dst = None, None, None, None\n",
    "        \n",
    "        html = \"\"\"<style>\n",
    "                div.cont {display:inline-block; margin:5px; vertical-align: top;}\n",
    "                table#t, table#t td, table#t tr {border: 1px dotted black; \n",
    "                                                background: white; padding: 1px;}\n",
    "                \n",
    "                table#t td {width: 1cm; height:1cm; text-align: center;}\n",
    "                table#t tr td.l {border-left: 2px solid red ;}\n",
    "                table#t tr td.r {border-right: 2px solid red ;}\n",
    "                table#t tr td.b {border-bottom: 2px solid red ;}\n",
    "                table#t tr td.t {border-top: 2px solid red ;}\n",
    "                table#t tr td.arret {background: yellow;}\n",
    "                </style>\n",
    "                <div class=\"cont\">\n",
    "                <table id=\"t\">\n",
    "                \"\"\"\n",
    "        for i in range(self.nb_l):\n",
    "            html += \"<tr>\"\n",
    "            for j in range(self.nb_c):\n",
    "                cls = None\n",
    "                html += \"<td \"\n",
    "                if (i, j) in self.bar:\n",
    "                    cls = 'class=\"'\n",
    "                    bl = self.bar[(i, j)]\n",
    "                    for b in bl:\n",
    "                        cls += bordures[b] + ' '\n",
    "                if (i, j) in self.arrets:\n",
    "                    if not cls:\n",
    "                        cls = 'class=\"'\n",
    "                    cls += 'arret'\n",
    "                if cls:\n",
    "                    cls += '\"'\n",
    "                    html += cls\n",
    "                html += '>'\n",
    "                cont = ''\n",
    "                if dst != None and self.arrets[dst] == (i, j):\n",
    "                    cont = '🏲'\n",
    "                if psg != None and psg != nb_a and self.arrets[psg] == (i, j):\n",
    "                    cont += '👽'\n",
    "                if (l, c) == (i, j):\n",
    "                    if psg != None and psg != nb_a:\n",
    "                        cont += '🚖'\n",
    "                    else:\n",
    "                        cont += '🚍'\n",
    "                if not cont:\n",
    "                    cont = ':'\n",
    "                html += cont + '</td>'\n",
    "            html += '</tr>'\n",
    "            \n",
    "        html += '</table></div>'\n",
    "        \n",
    "        return html\n",
    "        \n",
    "print('FIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "                div.cont {display:inline-block; margin:5px; vertical-align: top;}\n",
       "                table#t, table#t td, table#t tr {border: 1px dotted black; \n",
       "                                                background: white; padding: 1px;}\n",
       "                \n",
       "                table#t td {width: 1cm; height:1cm; text-align: center;}\n",
       "                table#t tr td.l {border-left: 2px solid red ;}\n",
       "                table#t tr td.r {border-right: 2px solid red ;}\n",
       "                table#t tr td.b {border-bottom: 2px solid red ;}\n",
       "                table#t tr td.t {border-top: 2px solid red ;}\n",
       "                table#t tr td.arret {background: yellow;}\n",
       "                </style>\n",
       "                <div class=\"cont\">\n",
       "                <table id=\"t\">\n",
       "                <tr><td class=\"l t arret\">🏲👽🚖</td><td class=\"r t \">:</td><td class=\"l t \">:</td><td class=\"t \">:</td><td class=\"r t arret\">:</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td></tr><tr><td class=\"l r b arret\">:</td><td class=\"l b \">:</td><td class=\"r b \">:</td><td class=\"l b arret\">:</td><td class=\"r b \">:</td></tr></table></div><div class=\"cont\"><p>Etape: 398</p><p>Etat: 16</p><p>Action: deposer</p><p>Récompense: 19</p><p>Récompense totale: -1068</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arrets = [(0,0), (0,4), (4,0), (4,3)]\n",
    "barrieres = {\n",
    "    (0, 1): set([1]), # barrière à droit\n",
    "    (0, 2): set([0]), # barrière à gauche\n",
    "    (3, 0): set([1]), # barrière à droit\n",
    "    (4, 0): set([1]), # barrière à droit\n",
    "    (3, 1): set([0]), # barrière à gauche\n",
    "    (4, 1): set([0]), # barrière à gauche\n",
    "    (3, 2): set([1]), # barrière à droit\n",
    "    (4, 2): set([1]), # barrière à droit\n",
    "    (3, 3): set([0]), # barrière à gauche\n",
    "    (4, 3): set([0]), # barrière à gauche\n",
    "}\n",
    "\n",
    "taxi = TaxiEnv(5, 5, arrets, barrieres)\n",
    "taxi.ajouter_agent(0.1, 0.1)\n",
    "taxi.initialiser((3, 1), 2, 0)\n",
    "\n",
    "\n",
    "html = taxi.dessiner()\n",
    "display(HTML(html))\n",
    "hist = taxi.transporter(plot=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "                div.cont {display:inline-block; margin:5px; vertical-align: top;}\n",
       "                table#t, table#t td, table#t tr {border: 1px dotted black; \n",
       "                                                background: white; padding: 1px;}\n",
       "                \n",
       "                table#t td {width: 1cm; height:1cm; text-align: center;}\n",
       "                table#t tr td.l {border-left: 2px solid red ;}\n",
       "                table#t tr td.r {border-right: 2px solid red ;}\n",
       "                table#t tr td.b {border-bottom: 2px solid red ;}\n",
       "                table#t tr td.t {border-top: 2px solid red ;}\n",
       "                table#t tr td.arret {background: yellow;}\n",
       "                </style>\n",
       "                <div class=\"cont\">\n",
       "                <table id=\"t\">\n",
       "                <tr><td class=\"l t arret\">🏲👽🚖</td><td class=\"r t \">:</td><td class=\"l t \">:</td><td class=\"t \">:</td><td class=\"r t arret\">:</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td></tr><tr><td class=\"l r b arret\">:</td><td class=\"l b \">:</td><td class=\"r b \">:</td><td class=\"l b arret\">:</td><td class=\"r b \">:</td></tr></table></div><div class=\"cont\"><p>Etape: 11</p><p>Etat: 16</p><p>Action: deposer</p><p>Récompense: 19</p><p>Récompense totale: 9</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tester après l'exécution de la même initialisation plusieurs fois\n",
    "for i in range(1000):\n",
    "    taxi.initialiser((3, 1), 2, 0)\n",
    "    taxi.transporter() \n",
    "\n",
    "taxi.initialiser((3, 1), 2, 0)\n",
    "hist = taxi.transporter(plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((3, 1, 2, 0), 'arriere', -1, False),\n",
       " ((2, 1, 2, 0), 'gauche', -1, False),\n",
       " ((2, 0, 2, 0), 'avant', -1, False),\n",
       " ((3, 0, 2, 0), 'avant', -1, False),\n",
       " ((4, 0, 2, 0), 'prendre', -1, False),\n",
       " ((4, 0, 4, 0), 'gauche', -1, False),\n",
       " ((4, 0, 4, 0), 'arriere', -1, False),\n",
       " ((3, 0, 4, 0), 'arriere', -1, False),\n",
       " ((2, 0, 4, 0), 'arriere', -1, False),\n",
       " ((1, 0, 4, 0), 'arriere', -1, False),\n",
       " ((0, 0, 4, 0), 'deposer', 19, True)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# historique des étapes\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n"
     ]
    }
   ],
   "source": [
    "# tester l'appentissage avec des initialisations aléatoires\n",
    "arrets2 = [(0,0), (0,4), (4,0), (4,3)]\n",
    "barrieres2 = {\n",
    "    (0, 1): set([1]), # barrière à droit\n",
    "    (0, 2): set([0]), # barrière à gauche\n",
    "    (3, 0): set([1]), # barrière à droit\n",
    "    (4, 0): set([1]), # barrière à droit\n",
    "    (3, 1): set([0]), # barrière à gauche\n",
    "    (4, 1): set([0]), # barrière à gauche\n",
    "    (3, 2): set([1]), # barrière à droit\n",
    "    (4, 2): set([1]), # barrière à droit\n",
    "    (3, 3): set([0]), # barrière à gauche\n",
    "    (4, 3): set([0]), # barrière à gauche\n",
    "}\n",
    "\n",
    "taxi2 = TaxiEnv(5, 5, arrets2, barrieres2)\n",
    "taxi2.ajouter_agent(0.1, 0.1)\n",
    "\n",
    "def exec_aleatoire(taxi_env, plot=False):\n",
    "    pos = np.random.randint(5), np.random.randint(5)\n",
    "    psg, dst = np.random.randint(len(arrets2)), np.random.randint(len(arrets2))\n",
    "    taxi_env.initialiser(pos, psg, dst)\n",
    "    return taxi_env.transporter(plot=plot) \n",
    "\n",
    "for i in range(10000):\n",
    "    exec_aleatoire(taxi2, plot=False)\n",
    "    \n",
    "print('fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "                div.cont {display:inline-block; margin:5px; vertical-align: top;}\n",
       "                table#t, table#t td, table#t tr {border: 1px dotted black; \n",
       "                                                background: white; padding: 1px;}\n",
       "                \n",
       "                table#t td {width: 1cm; height:1cm; text-align: center;}\n",
       "                table#t tr td.l {border-left: 2px solid red ;}\n",
       "                table#t tr td.r {border-right: 2px solid red ;}\n",
       "                table#t tr td.b {border-bottom: 2px solid red ;}\n",
       "                table#t tr td.t {border-top: 2px solid red ;}\n",
       "                table#t tr td.arret {background: yellow;}\n",
       "                </style>\n",
       "                <div class=\"cont\">\n",
       "                <table id=\"t\">\n",
       "                <tr><td class=\"l t arret\">:</td><td class=\"r t \">:</td><td class=\"l t \">:</td><td class=\"t \">:</td><td class=\"r t arret\">🏲👽🚖</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td></tr><tr><td class=\"l r b arret\">:</td><td class=\"l b \">:</td><td class=\"r b \">:</td><td class=\"l b arret\">:</td><td class=\"r b \">:</td></tr></table></div><div class=\"cont\"><p>Etape: 14</p><p>Etat: 97</p><p>Action: deposer</p><p>Récompense: 19</p><p>Récompense totale: -14</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = exec_aleatoire(taxi2, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Application et analyse\n",
    "\n",
    "Voici quelques outils pour tester l'apprentissage par renforcement :\n",
    "\n",
    "- OpenAI Baselines: https://github.com/openai/baselines\n",
    "- Intel Coach: https://github.com/IntelLabs/coach\n",
    "- Stable Baselines: https://github.com/DLR-RM/stable-baselines3\n",
    "- TF-Agents: https://github.com/tensorflow/agents\n",
    "- Keras-RL: https://github.com/keras-rl/keras-rl\n",
    "- Tensorforce: https://github.com/tensorforce/tensorforce\n",
    "- Chainer RL: https://github.com/chainer/chainerrl\n",
    "- Mushroom RL: https://github.com/MushroomRL/mushroom-rl\n",
    "- Acme: https://github.com/deepmind/acme\n",
    "- Dopamine: https://github.com/google/dopamine\n",
    "- RAY: https://github.com/ray-project/ray\n",
    "\n",
    "Environnements :\n",
    "\n",
    "- Gym: https://gym.openai.com/\n",
    "- iGibson: http://svl.stanford.edu/igibson/\n",
    "\n",
    "On va utiliser \"MushroomRL\" puisque l'outil implémente les méthodes traditionnelles.\n",
    "Aussi, on va utiliser \"Gym\" pour générer les environnements. \n",
    "L'environnement Taxi sera utilisé puisqu'il ne consomme pas beacoup de ressources (mémoire et calcul).\n",
    "Vous pouvez consulter \"Gym\" pour des environements plus complexes comme les jeux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# installation des packages\n",
    "!pip install gym\n",
    "!pip install mushroom_rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1. Exploration vs. Exploitation\n",
    "\n",
    "Ici, nous voulons tester l'effet de l'exploration/exploitation. \n",
    "Pour ce faire, nous allons tester avec des valeurs différentes de epsilon :\n",
    "- 0: exploitation (toujours)\n",
    "- 0.5: exploitation (50%) et exploration (50%)\n",
    "- 0.9: exploration (90%) et exploitation (10%).\n",
    "\n",
    "Le nombre max, par défaut, des étapes est fixé à 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mushroom_rl.core import Environment\n",
    "from mushroom_rl.policy import EpsGreedy\n",
    "from mushroom_rl.algorithms.value import QLearning\n",
    "from mushroom_rl.utils.dataset import compute_J\n",
    "from mushroom_rl.utils.parameters import Parameter\n",
    "from mushroom_rl.core import Core\n",
    "from mushroom_rl.utils.callbacks import CollectDataset\n",
    "from mushroom_rl.utils.callbacks.callback import Callback\n",
    "from mushroom_rl.utils.dataset import parse_dataset, episodes_length\n",
    "\n",
    "NBR_EPISODES = 1000 # nombre des exécutions\n",
    "\n",
    "env = Environment.make('Gym', 'Taxi-v3')\n",
    "\n",
    "epsilons = [.0, .5, .9]\n",
    "\n",
    "tests = []\n",
    "\n",
    "for eps in epsilons:\n",
    "    epsilon = Parameter(value=eps)\n",
    "    pi = EpsGreedy(epsilon=epsilon)\n",
    "    agent = QLearning(env.info, pi, learning_rate=Parameter(value=.3))\n",
    "    \n",
    "    collect_dataset = CollectDataset()\n",
    "    callbacks = [collect_dataset]\n",
    "    \n",
    "    core = Core(agent, env, callbacks_fit=callbacks)\n",
    "    core.learn(n_episodes=NBR_EPISODES, n_steps_per_fit=1)\n",
    "    \n",
    "    res = {}\n",
    "    res['nbr_etapes'] = episodes_length(collect_dataset.get())\n",
    "    tests.append(res)\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for i, test in enumerate(tests):\n",
    "    plt.plot(test['nbr_etapes'], label='epsilon=' + str(epsilons[i]))\n",
    "plt.xlabel('Episodes') \n",
    "plt.ylabel('Nombre des etapes') \n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyser les résultats** \n",
    "\n",
    "- Pourquoi l'algorithme avec plus d'exploration ne peut pas minimiser le nombre des étapes après plusieurs épisodes ?\n",
    "- Pourquoi il existe des épisodes qui ont un nombre minimal des étapes surtout dans les dernières épisodes (toujours dans l'algorithme avec plus d'exploration) ?\n",
    "- Pourquoi celui ui utilise seulement l'exploitation prend moins d'étapes à chaque épisode ?\n",
    "- Dans ce cas, quel est l'interêt de l'exploration ?\n",
    "\n",
    "**Réponse**\n",
    "\n",
    "- L’agent continue à explorer au lieu d’exploiter ce qu’il a appris. Il ne se stabilise pas sur le chemin optimal puisqu'il teste constamment de nouvelles actions, ce qui l’empêche de réduire efficacement le nombre d’étapes.\n",
    "-  Avec le temps, l'agent accumule suffisamment d'expérience pour exploiter une trajectoire optimale. \n",
    "- En exploitant uniquement la meilleure action connue, l’agent suit directement le chemin optimal qu’il a découvert, sans tester d'autres alternatives, ce qui réduit le nombre d'étapes.\n",
    "- L’exploration permet d’éviter de rester bloqué dans une solution sous-optimale en découvrant de meilleures stratégies. Elle est aussi essentielle pour s’adapter aux changements de l’environnement ou pour apprendre efficacement dans un nouvel environnement inconnu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2. Taux d'apprentissage\n",
    "\n",
    "Ici, nous voulons tester l'effet du taux d'apprentissage sur le nombre des itérations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBR_EPISODES = 1000 # nombre des exécutions\n",
    "\n",
    "env = Environment.make('Gym', 'Taxi-v3')\n",
    "\n",
    "lrs = [.1, .2, .3]\n",
    "\n",
    "tests = []\n",
    "\n",
    "for lr in lrs:\n",
    "    epsilon = Parameter(value=0.1)\n",
    "    pi = EpsGreedy(epsilon=epsilon)\n",
    "    agent = QLearning(env.info, pi, learning_rate=Parameter(value=lr))\n",
    "    \n",
    "    collect_dataset = CollectDataset()\n",
    "    callbacks = [collect_dataset]\n",
    "    \n",
    "    core = Core(agent, env, callbacks_fit=callbacks)\n",
    "    core.learn(n_episodes=NBR_EPISODES, n_steps_per_fit=1)\n",
    "    \n",
    "    res = {}\n",
    "    res['nbr_etapes'] = episodes_length(collect_dataset.get())\n",
    "    tests.append(res)\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for i, test in enumerate(tests):\n",
    "    plt.plot(test['nbr_etapes'], label='Taux=' + str(lrs[i]))\n",
    "plt.xlabel('Episodes') \n",
    "plt.ylabel('Nombre des etapes') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyser les résultats** \n",
    "\n",
    "- Quel est l'effet de $\\alpha$ sur le nombre des étapes après chaque $n$ épisodes ?\n",
    "- En consultant ce diagramme et celui avant, nous pouvons dire que l'évolution avec $\\epsilon=0$ est presque comme celle avec $\\alpha=0.3$ et l'évolution avec $\\epsilon=0.5$ est presque comme celle avec $\\alpha=0.1$. Dans ce cas, est ce que nous pouvons dire qu'il y ait une relation directe entre les deux paramètres (un peut etre remplacé avec une fonction sur l'autre) ? Pourquoi ?\n",
    "\n",
    "**Réponse**\n",
    "\n",
    "- Un $\\alpha$ plus élevé favorise des mises à jour rapides des valeurs d’action, permettant une adaptation plus rapide mais aussi plus d’instabilité. Un $\\alpha$ plus faible conduit à une convergence plus lente mais plus stable, impactant directement le nombre d’étapes nécessaires pour apprendre une politique optimale.\n",
    "- Non, il n’existe pas de relation directe car $\\alpha$ contrôle la vitesse d’apprentissage, tandis que $\\epsilon$ gère l’équilibre entre exploration et exploitation. Même si certaines valeurs donnent des évolutions similaires, leur rôle reste fondamentalement différent : $\\alpha$ agit sur la mise à jour des valeurs apprises, alors que $\\epsilon$ influence les décisions prises à chaque étape."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
